{"cells": [{"cell_type": "markdown", "id": "251d2b95-012d-4751-a240-e62efe81f77c", "metadata": {}, "source": "# Ajout des normales pour les temp\u00e9ratures et les pressions."}, {"cell_type": "markdown", "id": "7ddad129-3b54-453b-9e40-5af95f96c6a3", "metadata": {}, "source": "**Auteur :**  Steve Caron  \n**Date de cr\u00e9ation :** 2023/08/29  \n**Pr\u00e9sentation :** Ce notebook permet de charger des donn\u00e9es m\u00e9t\u00e9orologique depuis une table bigQuery, il calcul les normales pour deux mesures pour chaque jour de l'ann\u00e9e pour chaque station. Les valeurs normales, minimums et maximums sont ensuite ajouter \u00e0 la table de base. Le tout est sauvegard\u00e9 dans une table BigQuery.\n\n\n**Pr\u00e9requis :**  \n* Une table BigQuery contenant les donn\u00e9es m\u00e9t\u00e9orologique (TABLE_ID_INPUT)\n\n\n**Inputs :** \n\n**Params:**\n* TABLE_ID_INPUT : ID de la table BigQuery, doit \u00eatre dans le m\u00eame projet.\n* Table_ID_OUTPUT : ID de la table BigQuery, doit \u00eatre dans le m\u00eame projet.\n* LISTE_GROUPBY_INPUT : Le premier element de la liste DOIT ETRE le nom de la colonne avec la date.\n* NOM_COLONNE_DATE_AGGREGATION : Nouveaux nom de la colonne date avec laquelle faire l'aggr\u00e9gation de donn\u00e9e (exemple: jour_mois, mois, annee)\n* FORMAT_DATE_AGGREGATION : Format de la colonne NOM_COLONNE_DATE_AGGREGATION (exemple : dd-MM, MM, yyyy)\n* LISTE_COLONNES_A_NORMEES : Liste des colonnes sur lesquelles calculer la norme, doit contenir deux noms de colonne."}, {"cell_type": "code", "execution_count": 1, "id": "8e17671c-5fc5-4146-b405-2b2d9c204dcb", "metadata": {}, "outputs": [], "source": "from google.cloud import bigquery, storage\nfrom pyspark.sql.functions import col,avg,max,min, date_format\nfrom pyspark.sql.types import DecimalType"}, {"cell_type": "code", "execution_count": 2, "id": "5968f552-d479-429e-bba6-b36f8b33a4ef", "metadata": {}, "outputs": [], "source": "ID_TABLE_INPUT = \"code-de-source.donnees_code_de_source.donnees_meteo_test\"\nID_TABLE_OUTPUT = \"code-de-source.donnees_code_de_source.normale_meteo_test_norme\"\nLISTE_GROUPBY_INPUT = [\"date\",\"localite\"]\nNOM_COLONNE_DATE_AGGREGATION = \"jour-mois\"\nFORMAT_DATE_AGGREGATION = \"dd-MM\"\nLISTE_COLONNES_A_NORMEES = [\"precipitation\",\"temperature\"]"}, {"cell_type": "markdown", "id": "2a7e9e93-82c1-4cc8-a1f2-dbf863fb0f52", "metadata": {}, "source": "# Collecte des donn\u00e9es depuis une table Bigquery"}, {"cell_type": "code", "execution_count": 3, "id": "6648c067-e128-407f-9767-23bbc35dcf08", "metadata": {}, "outputs": [], "source": "donnees = spark.read.format('bigquery') \\\n  .option('table', ID_TABLE_INPUT) \\\n  .load()"}, {"cell_type": "markdown", "id": "623a3393-6423-47f9-a395-d4628e90685a", "metadata": {}, "source": "# Ajout de la colonnes date_aggregation"}, {"cell_type": "code", "execution_count": 4, "id": "0777fb21-e8bb-4ba0-8840-8fc13a4ac1b7", "metadata": {}, "outputs": [], "source": "data_jours_mois = donnees.withColumn(NOM_COLONNE_DATE_AGGREGATION,date_format(col(LISTE_GROUPBY_INPUT[0]), FORMAT_DATE_AGGREGATION))"}, {"cell_type": "markdown", "id": "f1a2a8d9-a8e6-4ac4-9b3d-956fcf4fcda5", "metadata": {}, "source": "# Ajout des colonnes normes."}, {"cell_type": "markdown", "id": "018cd50c-53c4-4e74-93ad-9818af826535", "metadata": {}, "source": "Pour chaque colonnes dont on veut calculer la norme, on creer une colonne moyenne,min,max"}, {"cell_type": "code", "execution_count": 5, "id": "78590db5-d5d6-42ef-988f-ec89738da27f", "metadata": {}, "outputs": [], "source": "liste_selection = LISTE_GROUPBY_INPUT[1:] + [NOM_COLONNE_DATE_AGGREGATION] + LISTE_COLONNES_A_NORMEES\n\n\ntable = data_jours_mois.select(liste_selection)\ntable = table.withColumn(f\"{LISTE_COLONNES_A_NORMEES[0]}_normale_min\",col(LISTE_COLONNES_A_NORMEES[0]))\\\n                    .withColumn(f\"{LISTE_COLONNES_A_NORMEES[0]}_normale_max\",col(LISTE_COLONNES_A_NORMEES[0]))\\\n                    .withColumnRenamed(LISTE_COLONNES_A_NORMEES[0],f\"{LISTE_COLONNES_A_NORMEES[0]}_normale\")\\\n                    .withColumn(f\"{LISTE_COLONNES_A_NORMEES[1]}_normale_min\",col(LISTE_COLONNES_A_NORMEES[1]))\\\n                    .withColumn(f\"{LISTE_COLONNES_A_NORMEES[1]}_normale_max\",col(LISTE_COLONNES_A_NORMEES[1]))\\\n                    .withColumnRenamed(LISTE_COLONNES_A_NORMEES[1],f\"{LISTE_COLONNES_A_NORMEES[1]}_normale\")\n"}, {"cell_type": "markdown", "id": "87565a18-e702-44fe-9078-39ff3bac0141", "metadata": {}, "source": "# Fait une aggr\u00e9gation de donn\u00e9es"}, {"cell_type": "code", "execution_count": 6, "id": "67749319-773a-49b7-ae3d-570d7f833f27", "metadata": {}, "outputs": [], "source": "# Je vais groupBy sur la liste LISTE_GROUPBY_INPUT, avec la premi\u00e8re colonne en moins (celle correspondant \u00e0 la date) et je la remplace par la colonne \"jour-mois\"\nListe_groupby = LISTE_GROUPBY_INPUT[1:]+[NOM_COLONNE_DATE_AGGREGATION]\n\ndonnees_aggregees = table.groupBy(Liste_groupby) \\\n    .agg(avg(f\"{LISTE_COLONNES_A_NORMEES[0]}_normale\").alias(f\"{LISTE_COLONNES_A_NORMEES[0]}_normale\"), \\\n         min(f\"{LISTE_COLONNES_A_NORMEES[0]}_normale_min\").alias(f\"{LISTE_COLONNES_A_NORMEES[0]}_normale_min\"), \\\n         max(f\"{LISTE_COLONNES_A_NORMEES[0]}_normale_max\").alias(f\"{LISTE_COLONNES_A_NORMEES[0]}_normale_max\"), \\\n         avg(f\"{LISTE_COLONNES_A_NORMEES[1]}_normale\").alias(f\"{LISTE_COLONNES_A_NORMEES[1]}_normale\"),\\\n         min(f\"{LISTE_COLONNES_A_NORMEES[1]}_normale_min\").alias(f\"{LISTE_COLONNES_A_NORMEES[1]}_normale_min\"), \\\n         max(f\"{LISTE_COLONNES_A_NORMEES[1]}_normale_max\").alias(f\"{LISTE_COLONNES_A_NORMEES[1]}_normale_max\"))"}, {"cell_type": "markdown", "id": "ec8341fa-f377-4bde-997e-44593a5178e6", "metadata": {"tags": []}, "source": "# Jointure sur ma table initiale"}, {"cell_type": "markdown", "id": "d2d0c113-4369-493f-ae30-954c753469d4", "metadata": {}, "source": "Je r\u00e9alise un left join sur les tables data_jours_mois & donnees_aggregees.  \nJe renomme les colonnes me permettant de faire la jointure sur la deuxi\u00e8me table pour pouvoir les supprimer facilement."}, {"cell_type": "code", "execution_count": 7, "id": "fc71fbe8-3d3d-41fa-bcaf-c4e05b1b9468", "metadata": {}, "outputs": [], "source": "nouveau_nom_col1 = f\"{LISTE_GROUPBY_INPUT[1]}_bis\"\nnouveau_nom_col_date = f\"{NOM_COLONNE_DATE_AGGREGATION}_bis\"\n\ndonnees_aggregees = donnees_aggregees.withColumnRenamed(LISTE_GROUPBY_INPUT[1],nouveau_nom_col1)\\\n                                    .withColumnRenamed(NOM_COLONNE_DATE_AGGREGATION,nouveau_nom_col_date)"}, {"cell_type": "code", "execution_count": 8, "id": "52e505bc-dc2c-485a-a226-3dd15e1f7b8c", "metadata": {}, "outputs": [], "source": "table_jointe = data_jours_mois.join(donnees_aggregees, (data_jours_mois[LISTE_GROUPBY_INPUT[1]] == donnees_aggregees[nouveau_nom_col1]) &\n                                    (data_jours_mois[NOM_COLONNE_DATE_AGGREGATION] == donnees_aggregees[nouveau_nom_col_date]), \"left\")"}, {"cell_type": "markdown", "id": "fed16663-c927-4ebd-92fd-b0c1f60c33b1", "metadata": {}, "source": "# Je supprime les colonnes en trop, resultantes de ma jointure"}, {"cell_type": "code", "execution_count": 9, "id": "2fe2b903-50c0-4cab-9710-e31cf13f0a3e", "metadata": {}, "outputs": [], "source": "donnees_finale = table_jointe.drop(col(nouveau_nom_col1)) \\\n                                .drop(col(nouveau_nom_col_date))"}, {"cell_type": "markdown", "id": "31311312-5df6-4f35-8861-7ce03b207c32", "metadata": {}, "source": "# Envoie de la table normale vers BigQuery"}, {"cell_type": "code", "execution_count": 10, "id": "9dbea916-e91b-4d1d-9f35-d3d4c8dbeb9e", "metadata": {}, "outputs": [], "source": "NOM_BUCKET_TEMP = \"temp-golden\"\nLOCATION = \"europe-west1\"\n\nstorage_client = storage.Client()\nbucket_temporaire = storage_client.create_bucket(NOM_BUCKET_TEMP, location=LOCATION)"}, {"cell_type": "code", "execution_count": 11, "id": "aa7970df-95e0-4912-8964-0840e0bf6f8f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "donnees_finale.write \\\n  .format(\"bigquery\") \\\n  .option(\"table\",ID_TABLE_OUTPUT)\\\n  .option(\"temporaryGcsBucket\",NOM_BUCKET_TEMP) \\\n  .mode(\"overwrite\") \\\n  .save()"}, {"cell_type": "code", "execution_count": 12, "id": "383a653e-9368-4ddc-8ace-b160e9bd062c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "bucket supprim\u00e9\n"}], "source": "# supprimer le bucket temporaire\nbucket_temporaire.delete(force=True)\nprint(\"bucket supprim\u00e9\")"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}