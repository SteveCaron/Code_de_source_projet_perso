{"cells": [{"cell_type": "markdown", "id": "2b27fadb-9ba7-4368-8cb1-d6e8599258a8", "metadata": {}, "source": "# Envoyer un CSV dans Big Querry"}, {"cell_type": "markdown", "id": "77e41971-28f1-4485-9804-c348c08827c2", "metadata": {}, "source": "**Auteur :**  Steve Caron  \n**Date de cr\u00e9ation :** 2023/07/25  \n**Pr\u00e9sentation :** Ce notebook permet de charger le contenu d'un CSV stock\u00e9 dans un bucket dans une table BigQuery. Il se lance depuis un dataproc appartenant au projet GCP.\n\n\n**Pr\u00e9requis :** \n* Un bucket gcp contenant les donn\u00e9es au format csv. (BUCKET_NAME)  \n* Une table bigQuery Dans le m\u00eame projet (TABLE_ID)  \n\n**Inputs :**  \n* fichier d'entr\u00e9: liste_station.csv  T\u00e9lechargeable ici : https://www.data.gouv.fr/fr/datasets/liste-des-stations-en-open-data-du-reseau-meteorologique-infoclimat-static-et-meteo-france-synop/\n\n**Params:** \n* BUCKET_NAME: nom du bucket contenant mes fichier source. Doit \u00eatre dans le m\u00eame projet\n* INPUT_FILE_NAME: nom du fichier a traiter. Doit \u00eatre au format csv\n* TABLE_ID: ID de la table dans BigQuerry. Doit \u00eatre dans le m\u00eame projet\n* PREMIERE_LIGNE : Param\u00e8tre permettant de dire si il faut charger la premi\u00e8re ligne du CSV dans la table. OUI = 1, NON = 0\n* AUTODETECTION : Param\u00e8tre du job config BigQuery, permettant d'activer ou non la detection automatique du schema (Activ\u00e9e = True, D\u00e9sactiv\u00e9e = False)"}, {"cell_type": "code", "execution_count": null, "id": "caa7e32a-c585-49f1-a5c5-b6653cbf274f", "metadata": {}, "outputs": [], "source": "from google.cloud import bigquery, storage\nimport os"}, {"cell_type": "code", "execution_count": null, "id": "d77ed019-0467-4536-a334-45747bcbecb3", "metadata": {}, "outputs": [], "source": "BUCKET_NAME = \"code_de_source_lake\"\nINPUT_FILE_NAME=\"liste_station.csv\"\nTABLE_ID = \"code-de-source.donnees_code_de_source.stations_meteo\"\nPREMIERE_LIGNE = 1\nAUTODETECTION = True\n\n\n\n#Je creer le chemin de mon fichier\nchemin_fichier_dataproc = f\"Data/{INPUT_FILE_NAME}\""}, {"cell_type": "markdown", "id": "8ba229c7-516a-4095-bcce-8b47833b53fb", "metadata": {}, "source": "# Telechargement du fichier depuis un bucket"}, {"cell_type": "code", "execution_count": null, "id": "fe92affe-fd7f-4c91-ba89-07ab28c1f59f", "metadata": {}, "outputs": [], "source": "# J'instancie un client storage\nstorage_client = storage.Client()\n# Je pr\u00e9cise le nom de bucket\nbucket = storage_client.bucket(BUCKET_NAME)\n#Dans un BLOB nomm\u00e9 mon_blob je charge le contenu de mon fichier\nmon_blob = bucket.blob(INPUT_FILE_NAME)\n\n\n# Je t\u00e9lecharge ensuite le contenu du BLOB nomm\u00e9 mon_blob dans un fichier\nmon_blob.download_to_filename(chemin_fichier_dataproc)\nprint(f\"Fichier CSV t\u00e9l\u00e9charg\u00e9 depuis le bucket {BUCKET_NAME} vers {INPUT_FILE_NAME}.\")"}, {"cell_type": "markdown", "id": "3a47cfa4-89ec-4226-aaa9-69a2fa822f13", "metadata": {}, "source": "# Envoie vers BigTable"}, {"cell_type": "code", "execution_count": null, "id": "e56df399-ebce-4430-9994-40cae58be83e", "metadata": {}, "outputs": [], "source": "# J'instancie un Client BigQuerry\nclient = bigquery.Client()\n\n# J'ouvre mon fichier qui est situ\u00e9 dans mon environnement de travail\nwith open(chemin_fichier_dataproc, \"rb\") as source_file:\n    # Je creer une configuration de JOB en pr\u00e9cisant le format et en sautant la premiere ligne  (header)\n    job_config = bigquery.LoadJobConfig(\n        autodetect= AUTODETECTION,\n        skip_leading_rows= PREMIERE_LIGNE,\n        source_format=bigquery.SourceFormat.CSV,\n    )\n    # Je lance mon job qui va envoyer le contenu de mon fichier source dans une table BigQuerry\n    job = client.load_table_from_file(source_file, TABLE_ID, job_config=job_config)\n\njob.result()\n\nprint(f\"Chargement du fichier {INPUT_FILE_NAME} dans la table {TABLE_ID} termin\u00e9.\")"}, {"cell_type": "markdown", "id": "b20b97cd-a3c0-4d8d-a61a-75ed2cf463e3", "metadata": {}, "source": "# Suppression du fichier"}, {"cell_type": "code", "execution_count": null, "id": "54883bab-e71d-4ffa-a508-c21d736479a0", "metadata": {}, "outputs": [], "source": "# Je v\u00e9rifie si le fichier existe dans le dataproc\nif os.path.exists(chemin_fichier_dataproc):\n    # Si il existe, alors je le supprime\n    os.remove(chemin_fichier_dataproc)\n    print(f\"Le fichier{INPUT_FILE_NAME} a \u00e9t\u00e9 correctement supprim\u00e9\")\nelse:\n    # Si il n'existe pas j'affiche le message suivant\n    print(f\"Impossible de supprimer le fichier {INPUT_FILE_NAME} car il n'existe pas\")"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}